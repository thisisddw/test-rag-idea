{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前目录下的modeling_llama.py被我进行了一点修改，它产生的kv_cache不包含位置信息，所以理论上可以进行kv_cache拼接。\n",
    "modeling_llama.py中被我修改的行都被注释`# (ddw)`标记。\n",
    "\n",
    "下面用一些简单输入对修改的模型和原版模型进行对比。看起来效果还可以，需要进一步测试。\n",
    "\n",
    "在总结三句话故事的任务中，修改模型用到了大部分信息，而原版模型只用了第一句话的信息。我把这视为修改过的模型拼接kv仍能提供位置信息的证据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ddw\\school\\研一\\研究方向调研\\repos\\test\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import LongTensor, FloatTensor\n",
    "\n",
    "# from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LlamaForCausalLM as OriginalLlamaForCausalLM\n",
    "from modeling_llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "model0 = OriginalLlamaForCausalLM.from_pretrained(checkpoint)\n",
    "model = LlamaForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefill(model: LlamaForCausalLM, input_ids: LongTensor) -> tuple[tuple[FloatTensor]]:\n",
    "    return model(input_ids).past_key_values\n",
    "\n",
    "def kv_cat(kv1: tuple[tuple[FloatTensor]], kv2: tuple[tuple[FloatTensor]]) -> tuple[tuple[FloatTensor]]:\n",
    "    # print(kv1[0][0].shape, kv2[0][0].shape)\n",
    "    n_layers = len(kv1)\n",
    "    \n",
    "    ret = tuple()\n",
    "    for i in range(n_layers):\n",
    "        k = torch.cat((kv1[i][0], kv2[i][0]), dim=2)\n",
    "        v = torch.cat((kv1[i][1], kv2[i][1]), dim=2)\n",
    "        ret = ret + ((k,v),)\n",
    "        \n",
    "    # print(ret[0][0].shape)\n",
    "    return ret\n",
    "\n",
    "def kv_slice(kv: tuple[tuple[FloatTensor]], l: int, r: int) -> tuple[tuple[FloatTensor]]:\n",
    "    n_layers = len(kv)\n",
    "    \n",
    "    ret = tuple()\n",
    "    for i in range(n_layers):\n",
    "        k = kv[i][0][:,:,l:r,:]\n",
    "        v = kv[i][1][:,:,l:r,:]\n",
    "        ret = ret + ((k,v),)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def kv_len(kv: tuple[tuple[FloatTensor]]) -> int:\n",
    "    return kv[0][0].shape[-2]\n",
    "    \n",
    "\n",
    "# def greedy_search(model: LlamaForCausalLM, \n",
    "#                 input_ids: LongTensor,\n",
    "#                 past_key_values: tuple[tuple[FloatTensor]],\n",
    "#                 eos_token_id: int,\n",
    "#                 max_length: int = 10) -> tuple[LongTensor, tuple[tuple[FloatTensor]]]:\n",
    "#     \"\"\"\n",
    "#     past_key_values may be None. The length of kv cache must less than or equal to \n",
    "#     the length of input_ids.\n",
    "\n",
    "#     Return (output_ids, new_key_values)\n",
    "#     \"\"\"\n",
    "#     if past_key_values is None:\n",
    "#         past_key_values = model(input_ids).past_key_values\n",
    "\n",
    "#     last_id = input_ids[0][-1]\n",
    "#     while len(input_ids[0]) < max_length and last_id != eos_token_id:\n",
    "#         model_outputs = model(LongTensor([[last_id]]), past_key_values=past_key_values)\n",
    "#         logits = model_outputs.logits\n",
    "#         past_key_values = model_outputs.past_key_values\n",
    "#         # print((input_ids.shape, torch.argmax(logits, dim=2)[:,-1].unsqueeze(0).shape))\n",
    "#         input_ids = torch.cat((input_ids, torch.argmax(logits, dim=2)[:,-1].unsqueeze(1)), dim=1)\n",
    "#         last_id = input_ids[0][-1]\n",
    "\n",
    "#     return input_ids, past_key_values\n",
    "\n",
    "\n",
    "# This function is largely due to ChatGPT. I only made a few changes.\n",
    "def greedy_search(model: LlamaForCausalLM, \n",
    "                  input_ids: LongTensor,\n",
    "                  past_key_values: tuple[tuple[FloatTensor]] = None,\n",
    "                  eos_token_id: int = 2,\n",
    "                  max_length: int = 10) -> tuple[LongTensor, tuple[tuple[FloatTensor]]]:\n",
    "    \"\"\"\n",
    "    Generates text using greedy search. Selects the most probable token at each step.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The causal language model to generate text with.\n",
    "    - input_ids: The input prompt tokens.\n",
    "    - past_key_values: Cache from previous forward pass (if available).\n",
    "    - eos_token_id: ID of the EOS token.\n",
    "    - max_length: Maximum number of tokens to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of output_ids (generated tokens) and new_key_values (updated cache).\n",
    "    \"\"\"\n",
    "    \n",
    "    input_len = len(input_ids[0])\n",
    "    if past_key_values:\n",
    "        input_kv_len = kv_len(past_key_values)\n",
    "    else:\n",
    "        input_kv_len = 0\n",
    "\n",
    "    assert input_kv_len <= input_len\n",
    "\n",
    "    if input_kv_len == input_len:\n",
    "        past_key_values = kv_slice(past_key_values, 0, input_kv_len - 1)\n",
    "        input_kv_len -= 1\n",
    "\n",
    "    # Initialize output with the initial input_ids\n",
    "    output_ids = input_ids\n",
    "    new_key_values = past_key_values  # To store updated past_key_values during generation\n",
    "\n",
    "    # Loop until max_length is reached or EOS token is generated\n",
    "    for _ in range(max_length):\n",
    "\n",
    "        # Forward pass through the model\n",
    "        # If past_key_values is None, we start without cache\n",
    "        outputs = model(input_ids=output_ids[:, input_kv_len - input_len:],  # Only feed the last token to avoid recalculating\n",
    "                        past_key_values=new_key_values,\n",
    "                        use_cache=True)\n",
    "        \n",
    "        # Update cache with new key/values\n",
    "        new_key_values = outputs.past_key_values\n",
    "\n",
    "        input_kv_len = kv_len(new_key_values)\n",
    "        input_len += 1\n",
    "\n",
    "        # Get the logits for the last generated token\n",
    "        logits = outputs.logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "        \n",
    "        # Select the token with the highest probability (greedy choice)\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        # Append to the generated sequence\n",
    "        output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "        \n",
    "        # Stop if EOS token is generated\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return output_ids, new_key_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT提供的测试用例。\n",
    "\n",
    "Sure! Here’s a story you can use for testing your chatbot’s summarization abilities. It has a mix of characters, events, and a bit of conflict, making it a good test for comprehension.\n",
    "\n",
    "---\n",
    "\n",
    "**Title:** *The Lost City of Aurelia*\n",
    "\n",
    "In the year 1897, a young archaeologist named Elara Thorne set off on an expedition to find the fabled Lost City of Aurelia, an ancient civilization rumored to be hidden deep within the Amazon rainforest. Elara, inspired by her grandfather’s stories of mythical cities, had spent years studying maps, analyzing ancient texts, and gathering clues. Along with a small team of explorers and a local guide named Mateo, she embarked on a journey filled with mystery and danger.\n",
    "\n",
    "The team ventured through treacherous terrain—raging rivers, dense forests, and poisonous creatures—to uncover any signs of the lost city. After weeks of struggle, they finally came across a series of stone markers etched with unfamiliar symbols. Elara recognized them from her grandfather’s notes and realized they were close.\n",
    "\n",
    "But as they ventured deeper, the team encountered a band of mercenaries led by an infamous treasure hunter named Victor Blackwood. Blackwood had been searching for Aurelia for years, driven by greed and a desire for fame. He threatened Elara and her team, demanding they share any information about the city’s location. Forced to comply but unwilling to give up, Elara secretly left false clues, leading Blackwood’s team in the wrong direction.\n",
    "\n",
    "Finally, after several tense days, Elara and her group arrived at a hidden waterfall. Behind it lay the entrance to a vast underground city, with towering stone pillars and golden statues—Aurelia at last! The city was everything Elara had dreamed of: a place of incredible beauty and history, with intricate carvings, forgotten technology, and treasures beyond imagination.\n",
    "\n",
    "But their joy was short-lived. Blackwood and his mercenaries found their way back, and a confrontation ensued. In a desperate attempt to protect the city, Elara and her team triggered a hidden mechanism that caused parts of the city to collapse, sealing off the main chambers and preserving Aurelia’s secrets. Blackwood, narrowly escaping, fled empty-handed.\n",
    "\n",
    "Elara left the jungle with only a few artifacts and her memories of the city. She knew that Aurelia’s true wealth was its history, and she wanted it to remain unspoiled. Returning to her university, she shared her findings with scholars but never revealed the city’s location, keeping its mysteries safe from the world.\n",
    "\n",
    "---\n",
    "\n",
    "This story has plenty of details for testing a summarizer: character motivations, a sequence of events, conflict, and a resolution. Good luck with your testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 191\n",
      "Modified model:\n",
      "system\n",
      "This is a piece of information that may be useful in question answering: In the year 1897, a young archaeologist named Elara Thorne set off on an expedition to find the fabled Lost City of Aurelia, an ancient civilization rumored to be hidden deep within the Amazon rainforest.\n",
      "system\n",
      "This is a piece of information that may be useful in question answering: Along with a small team of explorers and a local guide named Mateo, she embarked on a journey filled with mystery and danger.\n",
      "system\n",
      "This is a piece of information that may be useful in question answering: Elara left the jungle with only a few artifacts and her memories of the city. \n",
      "\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM. Please answer questions basing on the above infos.\n",
      "user\n",
      "Briefly Summarize the story of Elara.\n",
      "assistant\n",
      "Elara is a young archaeologist who embarks on a perilous journey to find the fabled Lost City of Aurelia, a city rumored to be hidden deep within the Amazon rainforest. Along the way, she encounters a local guide named Mateo, who helps her navigate the dense jungle terrain. As they explore the jungle, they uncover clues that suggest the city is not as abandoned as initially thought, and that there are still remnants of a powerful civilization.\n"
     ]
    }
   ],
   "source": [
    "def wrap_doc(doc: str) -> str:\n",
    "    return f\"\"\"<|im_start|>system\n",
    "This is a piece of information that may be useful in question answering: {doc}<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "docs = [\n",
    "    # \"Former Arkansas governor and Baptist minister Mike Huckabee has been named US ambassador to Israel.\",\n",
    "    # \"John Ratcliffe, a former Texas congressman and former director of national intelligence, has been nominated as the next CIA director.\",\n",
    "    # \"Fox News commentator and army veteran Pete Hegseth is Trump's pick to be the next US secretary of defence.\",\n",
    "    \n",
    "    # \"David Du is a master student of computer technology at Nankai University.\",\n",
    "    # \"David Du is a 22-year-old chinese national living in Tianjin, China.\",\n",
    "    # \"David Du is currently working on LLM inference systems.\",\n",
    "\n",
    "    \"In the year 1897, a young archaeologist named Elara Thorne set off on an expedition to find the fabled Lost City of Aurelia, an ancient civilization rumored to be hidden deep within the Amazon rainforest.\",\n",
    "    \"Along with a small team of explorers and a local guide named Mateo, she embarked on a journey filled with mystery and danger.\",\n",
    "    \"Elara left the jungle with only a few artifacts and her memories of the city. \",\n",
    "]\n",
    "docs = [wrap_doc(d) for d in docs]\n",
    "# docs = [\"\".join(docs)]\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM. Please answer questions basing on the above infos.<|im_end|>\n",
    "<|im_start|>user\n",
    "Who has been named US ambassador to Israel?<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM. Please answer questions basing on the above infos.<|im_end|>\n",
    "<|im_start|>user\n",
    "Tell me everything you know about David Du.<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM. Please answer questions basing on the above infos.<|im_end|>\n",
    "<|im_start|>user\n",
    "Briefly Summarize the story of Elara.<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "kv = None\n",
    "concated_input_ids = None\n",
    "for d in docs:\n",
    "    input = tokenizer(d, return_tensors=\"pt\").input_ids\n",
    "    kv = prefill(model, input) if kv is None else kv_cat(kv, prefill(model, input))\n",
    "    concated_input_ids = input if concated_input_ids is None else torch.cat((concated_input_ids, input), dim=1)\n",
    "\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# kv = kv_cat(kv, prefill(model, input))\n",
    "concated_input_ids = torch.cat((concated_input_ids, input), dim=1)\n",
    "\n",
    "print(kv_len(kv), concated_input_ids.shape[1])\n",
    "# kv = None\n",
    "\n",
    "print(\"Modified model:\")\n",
    "generate_ids, _ = greedy_search(model, concated_input_ids, kv, tokenizer.eos_token_id, 100)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 191\n",
      "Original model:\n",
      "system\n",
      "This is a piece of information that may be useful in question answering: In the year 1897, a young archaeologist named Elara Thorne set off on an expedition to find the fabled Lost City of Aurelia, an ancient civilization rumored to be hidden deep within the Amazon rainforest.\n",
      "system\n",
      "This is a piece of information that may be useful in question answering: Along with a small team of explorers and a local guide named Mateo, she embarked on a journey filled with mystery and danger.\n",
      "system\n",
      "This is a piece of information that may be useful in question answering: Elara left the jungle with only a few artifacts and her memories of the city. \n",
      "\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM. Please answer questions basing on the above infos.\n",
      "user\n",
      "Briefly Summarize the story of Elara.\n",
      "assistant\n",
      "Elara is a young archaeologist who embarks on an expedition to find the fabled Lost City of Aurelia, an ancient civilization rumored to be hidden deep within the Amazon rainforest.\n"
     ]
    }
   ],
   "source": [
    "# test original model (model0) in this block\n",
    "\n",
    "kv = None\n",
    "concated_input_ids = None\n",
    "for d in docs:\n",
    "    input = tokenizer(d, return_tensors=\"pt\").input_ids\n",
    "    kv = prefill(model0, input) if kv is None else kv_cat(kv, prefill(model0, input))\n",
    "    concated_input_ids = input if concated_input_ids is None else torch.cat((concated_input_ids, input), dim=1)\n",
    "\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# kv = kv_cat(kv, prefill(model0, input))\n",
    "concated_input_ids = torch.cat((concated_input_ids, input), dim=1)\n",
    "\n",
    "print(kv_len(kv), concated_input_ids.shape[1])\n",
    "# kv = None\n",
    "\n",
    "print(\"Original model:\")\n",
    "generate_ids, _ = greedy_search(model0, concated_input_ids, kv, tokenizer.eos_token_id, 150)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(outputs[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
